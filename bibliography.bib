
@book{sutton_reinforcement_2020,
	address = {Cambridge, Massachusetts London, England},
	edition = {Second edition},
	series = {Adaptive computation and machine learning},
	title = {Reinforcement learning: an introduction},
	isbn = {978-0-262-03924-6},
	shorttitle = {Reinforcement learning},
	abstract = {"Reinforcement learning, one of the most active research areas in artificial intelligence, is a computational approach to learning whereby an agent tries to maximize the total amount of reward it receives while interacting with a complex, uncertain environment. In Reinforcement Learning, Richard Sutton and Andrew Barto provide a clear and simple account of the field's key ideas and algorithms."--},
	language = {en},
	publisher = {The MIT Press},
	author = {Sutton, Richard S. and Barto, Andrew},
	year = {2020},
	file = {Sutton and Barto - 2020 - Reinforcement learning an introduction.pdf:/home/pierre/Zotero/storage/JJHRMHB3/Sutton and Barto - 2020 - Reinforcement learning an introduction.pdf:application/pdf},
}

@inproceedings{wiedebach_walking_2016,
	title = {Walking on {Partial} {Footholds} {Including} {Line} {Contacts} with the {Humanoid} {Robot} {Atlas}},
	url = {http://arxiv.org/abs/1607.08089},
	doi = {10.1109/HUMANOIDS.2016.7803439},
	abstract = {We present a method for humanoid robot walking on partial footholds such as small stepping stones and rocks with sharp surfaces. Our algorithm does not rely on prior knowledge of the foothold, but information about an expected foothold can be used to improve the stepping performance. After a step is taken, the robot explores the new contact surface by attempting to shift the center of pressure around the foot. The available foothold is inferred by the way in which the foot rotates about contact edges and/or by the achieved center of pressure locations on the foot during exploration. This estimated contact area is then used by a whole body momentum-based control algorithm. To walk and balance on partial footholds, we combine fast, dynamic stepping with the use of upper body angular momentum to regain balance. We applied this method to the Atlas humanoid designed by Boston Dynamics to walk over small contact surfaces, such as line and point contacts. We present experimental results and discuss performance limitations.},
	urldate = {2024-01-22},
	booktitle = {2016 {IEEE}-{RAS} 16th {International} {Conference} on {Humanoid} {Robots} ({Humanoids})},
	author = {Wiedebach, Georg and Bertrand, Sylvain and Wu, Tingfan and Fiorio, Luca and McCrory, Stephen and Griffin, Robert and Nori, Francesco and Pratt, Jerry},
	month = nov,
	year = {2016},
	note = {arXiv:1607.08089 [cs]},
	keywords = {Computer Science - Robotics},
	pages = {1312--1319},
	file = {arXiv Fulltext PDF:/home/pierre/Zotero/storage/2EEFJAMW/Wiedebach et al. - 2016 - Walking on Partial Footholds Including Line Contac.pdf:application/pdf;arXiv.org Snapshot:/home/pierre/Zotero/storage/I6KDXAIH/1607.html:text/html},
}

@article{silver_mastering_2016,
	title = {Mastering the game of {Go} with deep neural networks and tree search},
	volume = {529},
	copyright = {2016 Springer Nature Limited},
	issn = {1476-4687},
	url = {https://www.nature.com/articles/nature16961},
	doi = {10.1038/nature16961},
	abstract = {The game of Go has long been viewed as the most challenging of classic games for artificial intelligence owing to its enormous search space and the difficulty of evaluating board positions and moves. Here we introduce a new approach to computer Go that uses ‘value networks’ to evaluate board positions and ‘policy networks’ to select moves. These deep neural networks are trained by a novel combination of supervised learning from human expert games, and reinforcement learning from games of self-play. Without any lookahead search, the neural networks play Go at the level of state-of-the-art Monte Carlo tree search programs that simulate thousands of random games of self-play. We also introduce a new search algorithm that combines Monte Carlo simulation with value and policy networks. Using this search algorithm, our program AlphaGo achieved a 99.8\% winning rate against other Go programs, and defeated the human European Go champion by 5 games to 0. This is the first time that a computer program has defeated a human professional player in the full-sized game of Go, a feat previously thought to be at least a decade away.},
	language = {en},
	number = {7587},
	urldate = {2024-01-28},
	journal = {Nature},
	author = {Silver, David and Huang, Aja and Maddison, Chris J. and Guez, Arthur and Sifre, Laurent and van den Driessche, George and Schrittwieser, Julian and Antonoglou, Ioannis and Panneershelvam, Veda and Lanctot, Marc and Dieleman, Sander and Grewe, Dominik and Nham, John and Kalchbrenner, Nal and Sutskever, Ilya and Lillicrap, Timothy and Leach, Madeleine and Kavukcuoglu, Koray and Graepel, Thore and Hassabis, Demis},
	month = jan,
	year = {2016},
	note = {Number: 7587
Publisher: Nature Publishing Group},
	keywords = {Computational science, Computer science, Reward},
	pages = {484--489},
	file = {Full Text PDF:/home/pierre/Zotero/storage/SBMGI77X/Silver et al. - 2016 - Mastering the game of Go with deep neural networks.pdf:application/pdf},
}

@misc{amini_deep_2023,
	address = {Massachusetts Institute of Technology},
	title = {Deep {Reinforcement} {Learning}},
	url = {http://introtodeeplearning.com/2023/slides/6S191_MIT_DeepLearning_L6.pdf},
	author = {Amini, Alexander},
	month = jan,
	year = {2023},
}

@misc{grosse_q-learning_2019,
	address = {University of Toronto},
	title = {Q-{Learning}},
	url = {https://www.cs.toronto.edu/~rgrosse/courses/csc421_2019/slides/lec21.pdf},
	author = {Grosse, Roger and Ba, Jimmy},
	month = apr,
	year = {2019},
}
